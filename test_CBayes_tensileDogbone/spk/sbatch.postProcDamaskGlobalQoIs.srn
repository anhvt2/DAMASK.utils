#!/bin/bash
## Do not put any commands or blank lines before the #SBATCH lines
#SBATCH --nodes=1                     # Number of nodes - all cores per node are allocated to the job
#SBATCH --time=96:00:00               # Wall clock time (HH:MM:SS) - once the job exceeds this time, the job will be terminated (default is 5 minutes)
#SBATCH --account=FY140174            # WC ID
#SBATCH --job-name=cpfem              # Name of job
#SBATCH --partition=batch             # partition/queue name: short or batch
                                                                            #            short: 4hrs wallclock limit
                                                                            #            batch: nodes reserved for > 4hrs (default)
#SBATCH --qos=long                    # Quality of Service: long, large, priority or normal
                                                                            #           normal: request up to 48hrs wallclock (default)
                                                                            #           long:   request up to 96hrs wallclock and no larger than 64nodes
                                                                            #           large:  greater than 50% of cluster (special request)
                                                                            #           priority: High priority jobs (special request)


source ~/.bashrc
# ---------------------------------- set SRN modules
## compatible with DAMASK-2.0.2
module load gnu/10.2.1
module load openmpi-gnu/4.1
module load tce
module load python/3.6.0

# ---------------------------------- set DAMASK variables
# export PETSC_DIR=/ascldap/users/anhtran/data/local/petsc-3.9.4
export PETSC_DIR=/ascldap/users/anhtran/local/petsc-3.9.4 # DAMASK-2.0.2
# export PETSC_DIR=/ascldap/users/anhtran/local/petsc-3.10.5 # no longer at /data/ -- damask-2.0.3
# export PETSC_DIR=/ascldap/users/anhtran/local/petsc-3.13.6 # DAMASK-3.0.0-alpha
export PETSC_ARCH=arch-linux2-c-opt # could be arch-linux2-c-debug
export DAMASK_ROOT=/ascldap/users/anhtran/data/DAMASK/DAMASK-2.0.2
# export DAMASK_ROOT=/ascldap/users/anhtran/data/DAMASK/damask-2.0.3
# export DAMASK_ROOT=/ascldap/users/anhtran/data/DAMASK/damask-3.0.0-alpha
export DAMASK_spectral=$DAMASK_ROOT/bin/DAMASK_spectral

source /ascldap/users/anhtran/data/DAMASK/DAMASK-2.0.2/DAMASK_env.sh
# source /ascldap/users/anhtran/data/DAMASK/damask-3.0.0-alpha/env/DAMASK.sh

echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>"
echo "Start running simulation at"
echo $(date +%y-%m-%d-%H-%M-%S)

### ---------------------------------- pre-set on Solo

nodes=$SLURM_JOB_NUM_NODES           # Number of nodes - the number of nodes you have requested (for a list of SLURM environment variables see "man sbatch")
cores=10                             # Number MPI processes to run on each node (a.k.a. PPN)
                                                                         # CTS1 has 36 cores per node
### ---------------------------------- run DAMASK

geomFileName='main'
loadFileName='tension'

### ---------------------------------- post-processing DAMASK

# Global: Homogenization
rm -v postProc/${geomFileName}_${loadFileName}.txt
rm -v postProc/stress_strain.log
python3 ../../findGaugeLocations.py --geom ${geomFileName}.geom # dump gaugeFilter.txt
postResults *.spectralOut --cr f,p --filter $(cat gaugeFilter.txt)
cd postProc/
addStrainTensors ${geomFileName}_${loadFileName}.txt --left --logarithmic
addCauchy ${geomFileName}_${loadFileName}.txt
addMises ${geomFileName}_${loadFileName}.txt --strain 'ln(V)' --stress Cauchy
filterTable < ${geomFileName}_${loadFileName}.txt --white inc,'Mises(ln(V))','Mises(Cauchy)' > stress_strain.log
cd ..
