#!/bin/bash
## Do not put any commands or blank lines before the #SBATCH lines
#SBATCH --nodes=1                     # Number of nodes - all cores per node are allocated to the job
#SBATCH --time=4:00:00               # Wall clock time (HH:MM:SS) - once the job exceeds this time, the job will be terminated (default is 5 minutes)
#SBATCH --account=FY210060            # WC ID
#SBATCH --job-name=cpfem              # Name of job
#SBATCH --partition=short,batch       # partition/queue name: short or batch
																			#            short: 4hrs wallclock limit
																			#            batch: nodes reserved for > 4hrs (default)
#SBATCH --qos=normal                  # Quality of Service: long, large, priority or normal
																			#           normal: request up to 48hrs wallclock (default)
																			#           long:   request up to 96hrs wallclock and no larger than 64nodes
																			#           large:  greater than 50% of cluster (special request)
																			#           priority: High priority jobs (special request)


source ~/.bashrc

# ---------------------------------- set DAMASK variables
# export PETSC_DIR=/ascldap/users/anhtran/data/local/petsc-3.9.4
export PETSC_DIR=/ascldap/users/anhtran/local/petsc-3.9.4 # DAMASK-2.0.2
# export PETSC_DIR=/ascldap/users/anhtran/local/petsc-3.10.5 # no longer at /data/ -- damask-2.0.3
# export PETSC_DIR=/ascldap/users/anhtran/local/petsc-3.13.6 # DAMASK-3.0.0-alpha
export PETSC_ARCH=arch-linux2-c-opt # could be arch-linux2-c-debug
export DAMASK_ROOT=/ascldap/users/anhtran/data/DAMASK/DAMASK-2.0.2
# export DAMASK_ROOT=/ascldap/users/anhtran/data/DAMASK/damask-2.0.3
# export DAMASK_ROOT=/ascldap/users/anhtran/data/DAMASK/damask-3.0.0-alpha
export DAMASK_spectral=$DAMASK_ROOT/bin/DAMASK_spectral

source /ascldap/users/anhtran/data/DAMASK/DAMASK-2.0.2/DAMASK_env.sh
# source /ascldap/users/anhtran/data/DAMASK/damask-3.0.0-alpha/env/DAMASK.sh

echo ">>>>>>>>>>>>>>>>>>>>>>>>>>>"
echo "Start running simulation at"
echo $(date +%y-%m-%d-%H-%M-%S)

### ---------------------------------- pre-set on Solo

nodes=$SLURM_JOB_NUM_NODES           # Number of nodes - the number of nodes you have requested (for a list of SLURM environment variables see "man sbatch")
cores=36                             # Number MPI processes to run on each node (a.k.a. PPN)
																		 # CTS1 has 36 cores per node



# using openmpi-intel/1.10
# mpiexec --bind-to core --npernode $cores --n $(($cores*$nodes)) /path/to/executable [--args...]
# sh generateMsDream3d.sh # call DREAM.3D for microstructure generation

## NOTE: important to run 'parse2MaterialConfig.py' first to generate a correct material.config.preamble
## 		 that will be later on pasted to a subfoler below for coarse/fine meshes
python3 parse2MaterialConfig.py # parse inputs to material.config
numMs=5 # number of microstructures for Monte Carlo sample

# run DAMASK over an ensemble of microstructures
for i in $(seq ${numMs}); do
	# timestamp
	timeStamp=$(date +%Y-%m-%d-%H:%M:%S)
	logFile="query.log"
	echo "Start querying at" >> query.log
	echo ${timeStamp} >> query.log

	bash generateMsDream3d.sh # generate microstructures at different meshes

	mv 2x2x2 rve${i}_2x2x2
	mv 4x4x4 rve${i}_4x4x4
	mv 8x8x8 rve${i}_8x8x8
	mv 10x10x10 rve${i}_10x10x10
	mv 16x16x16 rve${i}_16x16x16
	mv 20x20x20 rve${i}_20x20x20
	mv 40x40x40 rve${i}_40x40x40
	mv 80x80x80 rve${i}_80x80x80

	### ---------------------------------- pre-process DAMASK
	# specify a mesh resolution to run simulation
	dimCell=8 # 40x40x40 is too computationally expensive; 8x8x8 takes ~49 minutes
	folderName=rve${i}_${dimCell}x${dimCell}x${dimCell}

	cd ${folderName}/
	### ---------------------------------- copy main file from the parent directory
	filePrefix="single_phase_equiaxed"
	cp ../tension.load  .
	# cp ../sigma.dat  .
	# cp ../mu.dat  .
	# cp ../msId.dat  .
	# cp ../material.config  .  # do not over-write material.config from generateMsDream3d.sh
	cp ../material.config.preamble  .
	# cp ../dimCell.dat  .
	# cp ../computeYieldStress.py  .
	cp ../${filePrefix}_${dimCell}x${dimCell}x${dimCell}.geom  .
	# ln -sf *.geom ${filePrefix}.geom # assumption: suppose that there is only one *.geom file
	
	rm -fv ${filePrefix}_${dimCell}x${dimCell}x${dimCell}_tension.* postProc/
	geom_check ${filePrefix}_${dimCell}x${dimCell}x${dimCell}.geom

	### ---------------------------------- run DAMASK
	# mpiexec --bind-to core --npernode $cores --n $(($cores*$nodes)) $DAMASK_spectral --geom ${filePrefix}.geom --load tension.load 2>&1 > log.damask

	if [ -f "numProcessors.dat" ]; then
		numProcessors=$(cat numProcessors.dat)
		if [[ "$numProcessors" -eq "1" ]]; then
			$DAMASK_spectral --geom ${filePrefix}_${dimCell}x${dimCell}x${dimCell}.geom --load tension.load 2>&1 > log.damask
		else
			mpirun -np ${numProcessors} $DAMASK_spectral --geom ${filePrefix}_${dimCell}x${dimCell}x${dimCell}.geom --load tension.load 2>&1 > log.damask
		fi
	else
		mpirun -np 36 $DAMASK_spectral --geom ${filePrefix}_${dimCell}x${dimCell}x${dimCell}.geom --load tension.load 2>&1 > log.damask
	fi


	### ---------------------------------- post-processing DAMASK
	sleep 10
	postResults *.spectralOut --cr f,p
	if [ -d "postProc" ]; then
		cd postProc/
		addStrainTensors ${filePrefix}_${dimCell}x${dimCell}x${dimCell}_tension.txt --left --logarithmic
		addCauchy ${filePrefix}_${dimCell}x${dimCell}x${dimCell}_tension.txt
		addMises ${filePrefix}_${dimCell}x${dimCell}x${dimCell}_tension.txt --strain 'ln(V)' --stress Cauchy
		addIPFcolor ${filePrefix}_${dimCell}x${dimCell}x${dimCell}_tension.txt --pole 0 0 1 --symmetry cubic # -o orientation
		filterTable < ${filePrefix}_${dimCell}x${dimCell}x${dimCell}_tension.txt --white inc,'Mises(ln(V))','Mises(Cauchy)' > stress_strain.log
		cd .. # rve${i}_${dimCell}x${dimCell}x${dimCell}
		cd .. # *_Iter
		python3 computeLossFunction.py --f=rve${i}_${dimCell}x${dimCell}x${dimCell}/ -p 1 # -p 0 if no plot, -p 1 with plt.savefig()
		if [ -f "output.dat" ]; then
			echo 1 > feasible.dat # to-be-replaced in computeLossFunction.py
			echo 1 > complete.dat
		fi
	else
		echo "Simulation does not converge!!!"
		echo 0 > feasible.dat # to-be-replaced in computeLossFunction.py
		echo 1 > complete.dat
	fi

	echo "<<<<<<<<<<<<<<<<<<<<<<<<<<<"
	echo "Stop running simulation at"
	echo $(date +%y-%m-%d-%H-%M-%S)

	# timestamp
	timeStamp=$(date +%Y-%m-%d-%H:%M:%S)
	echo "Stop querying at" >> query.log
	echo ${timeStamp} >> query.log

	echo "Done running DAMASK of RVE ${i} in $(pwd)"
done

python3 averageLossFunction.py
